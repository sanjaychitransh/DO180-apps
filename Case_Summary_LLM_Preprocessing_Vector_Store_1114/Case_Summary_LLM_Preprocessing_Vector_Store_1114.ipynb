{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cce8e57d-e7f5-4257-b3b4-7c0da6e16622",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "#!pip install openpyxl --upgrade\n",
    "import pandas as pd\n",
    "import re\n",
    "import ollama\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "97662c98-3b5b-412f-a285-7b36bd40a191",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Case: Case Number</th>\n",
       "      <th>Case: Subject</th>\n",
       "      <th>Case: Description</th>\n",
       "      <th>Case: Product Name</th>\n",
       "      <th>Automation Item Name</th>\n",
       "      <th>Created Date</th>\n",
       "      <th>Body</th>\n",
       "      <th>Feedback</th>\n",
       "      <th>Detailed Feedback</th>\n",
       "      <th>Case: Status</th>\n",
       "      <th>Case: Client Escalated</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TS017493063</td>\n",
       "      <td>*JLO* GBM-Kuwait-KOC-ESS- PROACTIVE ACTION AGA...</td>\n",
       "      <td>I am reaching out to discuss the IBM support a...</td>\n",
       "      <td>Elastic Storage System Hardware</td>\n",
       "      <td>AI-1559438</td>\n",
       "      <td>2024-10-13</td>\n",
       "      <td>Customer Sentiment negative Description Summar...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>IBM is working</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TS017525407</td>\n",
       "      <td>Email - We required high uninitialized Luns/ho...</td>\n",
       "      <td>We required high uninitialized Luns/hosts (Hig...</td>\n",
       "      <td>DS8900F</td>\n",
       "      <td>AI-1560218</td>\n",
       "      <td>2024-10-14</td>\n",
       "      <td>Customer Sentiment Not available Description S...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>good</td>\n",
       "      <td>Closed by IBM</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TS017529903</td>\n",
       "      <td>fca72a160 7/12 -&gt; fca72a163 7/8 QSFP port do n...</td>\n",
       "      <td>fca72a160 7/12 -&gt; fca72a163 7/8 QSFP port do n...</td>\n",
       "      <td>SAN b-type Collection</td>\n",
       "      <td>AI-1560770</td>\n",
       "      <td>2024-10-14</td>\n",
       "      <td>Customer Sentiment neutral Description Summary...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Closed by Client</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>TS014486964</td>\n",
       "      <td>SSD problem</td>\n",
       "      <td>Please note that there is a problem with one o...</td>\n",
       "      <td>Power System S914 Server</td>\n",
       "      <td>AI-1560820</td>\n",
       "      <td>2024-10-14</td>\n",
       "      <td>Customer Sentiment neutral Description Summary...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Awaiting your feedback</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>TS017567622</td>\n",
       "      <td>Blade unexpectedly powered off. No entries in ...</td>\n",
       "      <td>Synergy blade ohecp1esxi002 unexpectedly power...</td>\n",
       "      <td>Synergy Series Frames</td>\n",
       "      <td>AI-1561315</td>\n",
       "      <td>2024-10-14</td>\n",
       "      <td>Customer Sentiment negative Description Summar...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Closed by IBM</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Case: Case Number                                      Case: Subject  \\\n",
       "0       TS017493063  *JLO* GBM-Kuwait-KOC-ESS- PROACTIVE ACTION AGA...   \n",
       "1       TS017525407  Email - We required high uninitialized Luns/ho...   \n",
       "2       TS017529903  fca72a160 7/12 -> fca72a163 7/8 QSFP port do n...   \n",
       "3       TS014486964                                        SSD problem   \n",
       "4       TS017567622  Blade unexpectedly powered off. No entries in ...   \n",
       "\n",
       "                                   Case: Description  \\\n",
       "0  I am reaching out to discuss the IBM support a...   \n",
       "1  We required high uninitialized Luns/hosts (Hig...   \n",
       "2  fca72a160 7/12 -> fca72a163 7/8 QSFP port do n...   \n",
       "3  Please note that there is a problem with one o...   \n",
       "4  Synergy blade ohecp1esxi002 unexpectedly power...   \n",
       "\n",
       "                Case: Product Name Automation Item Name Created Date  \\\n",
       "0  Elastic Storage System Hardware           AI-1559438   2024-10-13   \n",
       "1                          DS8900F           AI-1560218   2024-10-14   \n",
       "2            SAN b-type Collection           AI-1560770   2024-10-14   \n",
       "3         Power System S914 Server           AI-1560820   2024-10-14   \n",
       "4            Synergy Series Frames           AI-1561315   2024-10-14   \n",
       "\n",
       "                                                Body Feedback  \\\n",
       "0  Customer Sentiment negative Description Summar...      NaN   \n",
       "1  Customer Sentiment Not available Description S...      NaN   \n",
       "2  Customer Sentiment neutral Description Summary...      NaN   \n",
       "3  Customer Sentiment neutral Description Summary...      NaN   \n",
       "4  Customer Sentiment negative Description Summar...      NaN   \n",
       "\n",
       "  Detailed Feedback            Case: Status  Case: Client Escalated  \n",
       "0               NaN          IBM is working                       0  \n",
       "1              good           Closed by IBM                       0  \n",
       "2               NaN        Closed by Client                       0  \n",
       "3               NaN  Awaiting your feedback                       0  \n",
       "4               NaN           Closed by IBM                       0  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read case summaries Excel file and create a DataFrame\n",
    "df = pd.read_excel(\"case_summarization.xlsx\")  \n",
    "df['Body']=df['Body'].apply(str)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "36f376c5-4541-4c66-b615-88edd39b4a80",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import wraps\n",
    "import shelve\n",
    "import hashlib\n",
    "\n",
    "def cached(func):\n",
    "    func.cache = shelve.open('llm_cache')\n",
    "    @wraps(func)\n",
    "    def wrapper(*args):\n",
    "        h = hashlib.sha512(str(args).encode('utf-8')).hexdigest()\n",
    "        try:\n",
    "            return func.cache[h]\n",
    "        except KeyError:\n",
    "            func.cache[h] = result = func(*args)\n",
    "            func.cache.close()\n",
    "            func.cache = shelve.open('llm_cache')\n",
    "            return result\n",
    "    return wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2e81ed90-6915-440d-8d10-eb2287f09728",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ID: TS017493063\n",
      "Subject: *JLO* GBM-Kuwait-KOC-ESS- PROACTIVE ACTION AGAINST THE IBM DOCUMENT NUMBER 7168392\n",
      "Description: I am reaching out to discuss the IBM support article regarding the IBM Storage Scale System 5000 with Power9 servers (MTM 5105-22E) running code 6.1.8.3-6.1.9.2, which indicates a high failure rate of NVDIMMs. Please schedule a call to discuss proactive measures and implementation plan. you can find the article here link:- https://www.ibm.com/support/pages/node/7168392?myns=swgother&mynp=OCSSP944&mync=E&cm_sp=swgother-_-OCSSP944-_-E\n",
      "Created Date: 2024-10-13 00:00:00\n",
      "Summary: Customer Sentiment negative Description Summary The customer is concerned about a high failure rate of NVDIMMs in IBM Storage Scale System 5000 with Power9 servers (MTM 5105-22E) running code 6.1.8.3-6.1.9.2. They are seeking a proactive call to discuss measures and an implementation plan. The relevant IBM support article can be found here: . Feed Summary Not available Resolution Summary Confirmed Solution Not Summarized Solution Attempts *Generated by watsonx*: There is a rpc-statd issue on node 'prot3'. The suggested resolution is to reboot 'prot3' using the commands 'mmces node suspend --stop -N ibmessprot3', 'mmshutdown -N ibmessprot3 -f', and 'reboot'. After the reboot, the node should be resumed using 'mmces node resume --start -N ibmessprot3' and the health status should be checked. Additionally, some Ganesha configuration tuning is recommended, such as modifying the 'MDCACHE' and 'NFS\\_Core\\_Param' blocks in the gpfs.ganesha.main.conf file and restarting the NFS Ganesha service on all protocol nodes.\n",
      "\n",
      "ID: TS017525407\n",
      "Subject: Email - We required high uninitialized Luns/hosts (High workload) and any hardware issues observed at storage end\n",
      "Description: We required high uninitialized Luns/hosts (High workload) and any hardware issues observed at storage end\n",
      "Created Date: 2024-10-14 00:00:00\n",
      "Summary: Customer Sentiment Not available Description Summary Not Summarized Email - We required high uninitialized Luns/hosts (High workload) and any hardware issues observed at storage end We required high uninitialized Luns/hosts (High workload) and any hardware issues observed at storage end Feed Summary Summary: ICICI Bank requested assistance with high uninitialized LUNs/hosts and potential hardware issues on their DS8900F storage system. IBM Support requested more information about the high uninitialized LUNs/hosts and any observed hardware issues. ICICI Bank provided a time frame for the issue and confirmed there were no open events or hardware issues during that time. They also requested information on which LSS/host were consuming high load during the specified time stamps. IBM Support recommended a bandwidth study of the existing dark fiber links between NDC/ODC and COLO sites and the configuration of Spectrum Control monitoring software to send reports about host workload metrics. IBM Expert Labs team can conduct bandwidth study and configure Spectrum Control reporting. IBM Support also requested spectrum control reports for the time frame 07-10-2024 10:00 - 17:00 hrs from all the 3 storage systems. ICICI Bank later provided spectrum control reports for two of the storage systems and requested analysis. IBM Support analyzed the reports and provided a table with the replication bandwidth for each system during the specified time stamps. The maximum combined send replication bandwidth for the two systems did not exceed 4Gbps, and the maximum send replication bandwidth for the third system did not exceed 2.5Gbps. IBM Support also noted SAN congestion on the metro mirror ports in all three DS8K systems, which could impact the write latency of host applications. They recommended engaging with the SAN Support team to analyze the buffer starvation issue on the metro mirror ports and engaging with link vendors to optimize the link latencies. ICICI Bank later requested information on the sessions running during the specified time frame, but IBM Support noted that session related information is not available in spectrum control performance package. Resolution Summary Confirmed Solution *Generated by watsonx*: The maximum combined send replication bandwidth on systems MR90 and VG20 is 441 MiBps. The link throughput from DC sites to COLO site does not exceed 4Gbps for the problem timestamps. The maximum send replication bandwidth on system MR80 is 280 MiBps. The link throughput from COLO site to DC sites does not exceed 2.5Gbps for the problem timestamps. However, there is SAN congestion on the metro mirror ports in all the 3 DS8K systems, which can have an impact on the write latency of the host applications. Recommendations to resolve the issue include engaging with the SAN Support team to analyze the buffer starvation issue on the metro mirror ports and engaging with link vendors to optimize the link latencies to be as close to each other as possible. No actions are recommended on the DS8K systems. Solution Attempts *Generated by watsonx*: The hardware analysis shows that all 3 DS8K storage systems are operating in optimal status during the specified problem window and there are no hardware issues that can lead to a performance problem. The support team has requested more information, including clarification of what is meant by high uninitialized LUNs/hosts, a list of affected lss/volume IDs/volume group/cluster/host on each DS8K system, and details about the impact to the application due to the reported performance problem. They have also asked for observations from the database and OS perspectives.\n",
      "\n",
      "ID: TS017529903\n",
      "Subject: fca72a160 7/12 -> fca72a163 7/8 QSFP port do not come online\n",
      "Description: fca72a160 7/12 -> fca72a163 7/8 QSFP port do not come online One port of a qsfp between fca72a160 7/12 -> fca72a163 7/8 do not come online. We checked already all cables. The cable are okay. Please send a QSFP with a CE onsite to replace it. Supportsave from both switches will follow. See case TS017126630 where Chris Rossow used wrap plug to find the right qsfp. Rgds Uwe\n",
      "Created Date: 2024-10-14 00:00:00\n",
      "Summary: Customer Sentiment neutral Description Summary The customer's QSFP port between two switches, fca72a160 7/12 -> fca72a163 7/8, is not coming online. The customer has checked all cables and found them to be okay. A replacement QSFP with a CE onsite is requested. The customer has also been advised to refer to case TS017126630 for troubleshooting guidance. Feed Summary The customer reported that one port of a QSFP between two switches was not coming online. The customer had already checked the cables and confirmed they were working correctly. The customer requested a QSFP with a CE onsite to replace it and provided the serial number of the affected asset. The support team analyzed the SMOG report and found errors in the port health for today on both switches. The team requested logs for fca72a163, which the customer provided. The team found that both switches were reporting a loss of signal and contacted IBM SSR Rossow C. The SSR replaced the SFP in fca76a163 and generated a summary of the case. Resolution Description WO07463087: replaced the sfp in fca76a163\n",
      "\n",
      "ID: TS014486964\n",
      "Subject: SSD problem\n",
      "Description: Please note that there is a problem with one of the SSDs with the below code and details. 2D35-FFF6 Explanation Device detected recoverable error. Response Note: Replacement of a read intensive SSD might not be covered by the system's level of service entitlement, depending on the terms and conditions of the system. For more information about read intensive SSDs, see Read intensive SSDs. Use MAP0210 : General Problem Resolution. Failing Item FFC_722 Failing function code 722 The disk drive might be failing. Use the following table to determine the part number for the field replaceable unit (FRU). CCIN or FFC Type and model Part number Description Location code Any Unknown disk drive\n",
      "Created Date: 2024-10-14 00:00:00\n",
      "Summary: Customer Sentiment neutral Description Summary The customer is experiencing a problem with an SSD (2D35-FFF6) that has detected a recoverable error. The customer service representative (CSR) suggested replacing the SSD due to the failing function code 722, which indicates a potential disk drive failure. However, the customer should verify if the replacement is covered by their system's service entitlement. The FRU part number for the SSD is unknown. Feed Summary A problem with an SSD was reported. The SSD has the code 2D35-FFF6 and is showing a recoverable error. The disk drive might be failing. The part number for the field replaceable unit (FRU) is unknown. IBM support requested a SNAP file for further investigation. The customer provided the SNAP file and it was analyzed. The analysis showed that disk drive pdisk0/hdisk0 is faulty and needs to be replaced. The disk is mirrored in rootvg and needs to be removed from the mirror and the volume group prior to replacement. IBM support requested intervention details for the replacement of the faulty disk drive. The customer provided the necessary details and the intervention was scheduled for next Sunday 29/10/2023. However, the parts for the replacement were delayed and the intervention was also delayed. The NAD was changed multiple times to reflect the delay in the intervention and parts delivery. The current NAD is set for Wednesday, 30 Oct 2024 10:00 AM EEST. Resolution Summary Not available\n",
      "\n",
      "ID: TS017567622\n",
      "Subject: Blade unexpectedly powered off. No entries in blades iLO or IML logs\n",
      "Description: Synergy blade ohecp1esxi002 unexpectedly powered off (x2). No entries in blades iLO or IML logs - I believe this is a reoccurrence of a recent failure. AHS logs are attached if required. Please open a new case for hardware investigation.\n",
      "Created Date: 2024-10-14 00:00:00\n",
      "Summary: Customer Sentiment negative Description Summary The customer's Blade, specifically ohecp1esxi002, unexpectedly powered off without any entries in the blades' iLO or IML logs. This issue has occurred before, and the customer has attached AHS logs for investigation. A new case has been opened for hardware investigation. Feed Summary Blade server experienced unexpected shutdowns, no relevant logs. Customer requested hardware investigation. IBM support identified the serial number and assigned a case to CDS B2B for hardware investigation. Customer provided the asset address, and CDS support requested confirmation of the motherboard location and part number. Resolution Summary Not available\n",
      "\n",
      "ID: TS017489632\n",
      "Subject: Error received: 6070 Description: cpqHeSysBatteryFailed:The HPE Smart Storage Battery failed.. Verifies SNMP configuration\n",
      "Description: ADDRESS IS NOT CONFIRMED Failing System Information: ========================== Machine Name: gbd21666-con.systems.uk.hsbc Machine Manufacturer: HP Machine Family: ProLiant DL380 Gen9 Registered Customer Number: 0317011 Service Location Code: SMH0A16 Machine Serial from system: CZ3638RF4S Machine Serial from config file: CZ3638RF4S Configured Product: HP ILO4 Unique Identifier: EPS/DALD01269396 Error Information: ========================== Error Code: 6070 Error Description: cpqHeSysBatteryFailed:The HPE Smart Storage Battery failed. Error Time: 2 Oct 2024 19:47:54 GMT Error Severity: Minor - 3 Local Information: ========================== Local problem id: 1ead8da542ef40fb9f44c6955b02fe4d eService system id: 189aa9f78e5b01cdca7e434985112a4b Source of information: ========================== IBM Electronic Service Agent (ESA) name: Service Agent - xESA ESA Version: 4.6.0.5 Operating System: Red Hat Enterprise Linux 8.10 Unique Identifier: EPS/DALD01269005 ESA Host Name: gbl25107906.hc.cloud.uk.hsbc Customer Information: ========================== Customer Enterprise Name: HSBC Customer Name for Technical Support: UNIX PSD Customer Phone for Technical Support: 123456789 Customer Email for Technical Support: dbsitiddcprodsupportunix@noexternalmail.hsbc.com Address where systems reside: Wakefield Group Data Centre, Wakefield, Normanton, WF6 1GY, Wakefield Group Data Centre, GB phone:123456789 Customer Name (Secondary) for Technical Support: Udayraj Naik Customer Phone (Secondary) for Technical Support: 123456789 Customer Email (Secondary) for Technical Support: udayraj.naik@hsbc.co.in Is Customer willing to communicate in English: Y\n",
      "Created Date: 2024-10-14 00:00:00\n",
      "Summary: Customer Sentiment neutral Description Summary The customer is experiencing an error with their HP ProLiant DL380 Gen9 machine, specifically the HPE Smart Storage Battery. The error code 6070, \"cpqHeSysBatteryFailed,\" indicates a failed battery. The customer has already verified the SNMP configuration. The system's serial number is CZ3638RF4S, and the configured product is HP ILO4. The error was reported on October 2, 2024, at 19:47:54 GMT. The customer's service location code is SMH0A16, and the system is located in the Wakefield Group Data Centre, Wakefield, Normanton, WF6 1GY, Wakefield, GB. The customer is willing to communicate in English. Feed Summary Summary: The customer reported an error related to a failed HPE Smart Storage Battery on a HP ProLiant DL380 Gen9 server. The error code was 6070, and the error description was \"cpqHeSysBatteryFailed:The HPE Smart Storage Battery failed.\" The error occurred on October 2, 2024, at 19:47:54 GMT. The customer provided logs and other necessary information to diagnose the issue. The failed battery was in a pre-failure state and needed to be replaced. The part number for the replacement battery was 878643-001. IBM arranged for the delivery of the replacement battery and scheduled an engineer to replace it. The delivery address was confirmed as Wakefield Group Data Centre, Premier Way North, Normanton, Wakefield, WF6 1GY. The server's location within the datacenter room was also provided, including the rack and slot numbers. IBM requested access to the server to replace the battery. The access was granted, and the battery was replaced. The server was powered on, and the customer confirmed that it was up and running with all green status on the console. The case was then closed after confirming that the issue was completely solved. Resolution Summary Confirmed Solution Not Summarized Solution Attempts *Generated by watsonx*: After checking them, we saw that the smart storage battery is in pre-failure state and needs to be replaced. In order to proceed with the replacement arrangements, the delivery address, specific requirements for the shipment, preferred date and time for the delivery, whether the address is the same as the delivery address, server's location within the datacenter room, including the rack and slot numbers, and preferred date and time for the intervention need to be confirmed. Downtime will be required.\n",
      "\n",
      "ID: TS017519395\n",
      "Subject: Error received: 3046 Description: cpqDa7PhyDrvStatusChange:Smart Array physical drive status change detected.. Verifies SNMP configuration.\n",
      "Description: Failing System Information: ========================== Machine Name: gbd18386-con.systems.uk.hsbc Machine Manufacturer: HP Machine Family: ProLiant DL380 Gen9 Registered Customer Number: 0246456 Service Location Code: SMH0A16 Machine Serial from system: CZ3602WSDX Machine Serial from config file: CZ3602WSDX Configured Product: HP ILO4 Unique Identifier: EPS/DALD01273726 Error Information: ========================== Error Code: 3046 Error Description: cpqDa7PhyDrvStatusChange:Smart Array physical drive status change detected. Error Time: 7 Oct 2024 11:07:57 GMT Error Severity: Minor - 3 Local Information: ========================== Local problem id: 35d88ef1df4a432b80dfab1bfe8e8592 eService system id: 534e634ea269ec472ec35bf142517b94 Source of information: ========================== IBM Electronic Service Agent (ESA) name: Service Agent - xESA ESA Version: 4.6.0.5 Operating System: Red Hat Enterprise Linux 8.10 Unique Identifier: EPS/DALD01269005 ESA Host Name: gbl25107906.hc.cloud.uk.hsbc Customer Information: ========================== Customer Enterprise Name: HSBC Customer Name for Technical Support: UNIX PSD Customer Phone for Technical Support: 123456789 Customer Email for Technical Support: dbsitiddcprodsupportunix@noexternalmail.hsbc.com Address where systems reside: Wakefield Group Data Centre, Wakefield, Normanton, WF6 1GY, Wakefield Group Data Centre, GB phone:123456789 Customer Name (Secondary) for Technical Support: Udayraj Naik Customer Phone (Secondary) for Technical Support: 123456789 Customer Email (Secondary) for Technical Support: udayraj.naik@hsbc.co.in Is Customer willing to communicate in English: Y\n",
      "Created Date: 2024-10-14 00:00:00\n",
      "Summary: Customer Sentiment neutral Description Summary The customer is experiencing an error with their HP ProLiant DL380 Gen9 machine, specifically with the Smart Array physical drive status change. The error code is 3046, and it was detected on October 7, 2024, at 11:07:57 GMT. The error is considered minor, and the local problem ID is 35d88ef1df4a432b80dfab1bfe8e8592. The issue was reported to IBM Electronic Service Agent (ESA) version 4.6.0.5, which is running on Red Hat Enterprise Linux 8.10. The customer's name for technical support is UNIX PSD, and they can be reached at 123456789 or dbsitiddcprodsupportunix@noexternalmail.hsbc.com. The customer is willing to communicate in English. Feed Summary A faulty drive was detected in a HP ProLiant DL380 Gen9 server with serial number CZ3602WSDX. The error was detected by the IBM Electronic Service Agent (ESA) tool and reported to the customer. The customer was asked to provide the ServiceNow ticket reference and upload the EED (Extended Error Data) report. The customer confirmed the failed drive and provided the necessary details for the replacement. The spare part, 872772-001 4TB hard disk drive, was ordered to the site for replacement. The replacement was scheduled for 9th OCT 2024 20:00 UKT to 24:00 UKT. The customer was informed about the engineer details and the cabinet key access required for the replacement. The disk was swapped after 20:00 as requested and the case was closed with customer confirmation. Resolution Summary Confirmed Solution Not Summarized Solution Attempts *Generated by watsonx*: This ticket is about: CSP case ref: TS017519395, Machine Name: gbd18386, Machine Serial from system: CZ3602WSDX, Error Description: cpqDa7PhyDrvStatusChange:Smart Array physical drive status change detected. Local problem id: 35d88ef1df4a432b80dfab1bfe8e8592, ESA Host Name: gbl25107906.hc.cloud.uk.hsbc, Location: Wakefield Group Data Centre, Wakefield, Normanton, WF6 1GY, Wakefield Group Data Centre, GB. The error indicates that the drive may require replacement. Awaiting EED report for confirmation. If replacement is required, please provide preferred date and time, address confirmation, and location of the system in the datacenter.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Process case summaries with LLM\n",
    "@cached\n",
    "def get_RCA_from_LLM(context, prompt, model):\n",
    "#The RCA must at least contain the root cause, analysis, resolution, and impact.\n",
    "    response = ollama.chat(\n",
    "        model=model,\n",
    "        messages=[{\n",
    "            'role': 'user',\n",
    "            'content': prompt + context\n",
    "        }]\n",
    "    )\n",
    "    return response['message']['content']\n",
    "\n",
    "# Loop through cases and generate RCA\n",
    "prompt = \"\"\"\n",
    "Here is an example of an RCA:\n",
    "{\n",
    "  \"RCA\": {\n",
    "    \"incident_id\": \"\",\n",
    "    \"date\": \"\",\n",
    "    \"problem\": {\n",
    "      \"description\": \"Memory leak causing service degradation in Service C\",\n",
    "      \"symptoms\": \"Service C experienced gradual performance degradation over 3 hours.\",\n",
    "      \"detected_by\": \"Monitoring system - High memory usage alert\"\n",
    "    },\n",
    "    \"root_cause\": {\n",
    "      \"description\": \"Improper memory management in the data processing module.\",\n",
    "      \"related_services\": [\"Service A\", \"Service B\"],\n",
    "      \"historical_pattern\": \"Similar memory leak issues found in Service A (Jan 2021) and Service B (Mar 2022).\"\n",
    "    },\n",
    "    \"resolution\": {\n",
    "      \"description\": \"Applied memory optimization techniques, including better memory allocation strategies and garbage collection triggers.\",\n",
    "      \"steps_taken\": [\n",
    "        \"Identified the memory leak using memory profiler tools.\",\n",
    "        \"Optimized memory usage in the data processing module.\",\n",
    "        \"Tested the fix in a controlled environment.\"\n",
    "      ],\n",
    "      \"time_to_resolve\": \"2 hours\",\n",
    "      \"service_downtime\": \"3 hours\",\n",
    "      \"service_restoration\": \"Full service restored after memory optimization.\"\n",
    "    },\n",
    "    \"impact\": {\n",
    "      \"description\": \"Service degradation for 3 hours, impacting 15% of users.\",\n",
    "      \"business_impact\": \"Moderate performance issues, leading to delays in processing large files.\"\n",
    "    },\n",
    "    \"preventive_actions\": {\n",
    "      \"description\": \"Implemented proactive memory monitoring and automated garbage collection.\",\n",
    "      \"actions\": [\n",
    "        \"Added memory usage alerts in the monitoring system.\",\n",
    "        \"Conducted code review to improve memory handling in related services.\"\n",
    "      ],\n",
    "      \"future_risk\": \"High likelihood of similar issues in Service D due to similar code structure.\"\n",
    "    }\n",
    "  }\n",
    "}\n",
    "Assume the role of an SRE engineer and give me an RCA report based on the following summary.\n",
    "Return the report in json format.\n",
    "If you don't have the data or it's unclear return an empty string :\\n\n",
    "\"\"\"\n",
    "RCA = []\n",
    "model='granite3-dense:8b-instruct-fp16'\n",
    "for i, row in df.iterrows():\n",
    "    context = 'ID: '+row['Case: Case Number']+\"\\n\"    \n",
    "    context += 'Subject: '+row['Case: Subject']+\"\\n\" \n",
    "    context += 'Description: '+row['Case: Description']+\"\\n\" \n",
    "    context += 'Created Date: '+str(row['Created Date'])+\"\\n\" \n",
    "    context += 'Summary: '+row['Body']+\"\\n\" \n",
    "    print(context)\n",
    "    RCA += [get_RCA_from_LLM(context, prompt, model)]\n",
    "    if i > 5: break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "be8c4a1d-a1fe-4ffa-b505-9a15df58f9ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Summary:  Customer Sentiment negative Description Summary The customer is concerned about a high failure rate of NVDIMMs in IBM Storage Scale System 5000 with Power9 servers (MTM 5105-22E) running code 6.1.8.3-6.1.9.2. They are seeking a proactive call to discuss measures and an implementation plan. The relevant IBM support article can be found here: . Feed Summary Not available Resolution Summary Confirmed Solution Not Summarized Solution Attempts *Generated by watsonx*: There is a rpc-statd issue on node 'prot3'. The suggested resolution is to reboot 'prot3' using the commands 'mmces node suspend --stop -N ibmessprot3', 'mmshutdown -N ibmessprot3 -f', and 'reboot'. After the reboot, the node should be resumed using 'mmces node resume --start -N ibmessprot3' and the health status should be checked. Additionally, some Ganesha configuration tuning is recommended, such as modifying the 'MDCACHE' and 'NFS\\_Core\\_Param' blocks in the gpfs.ganesha.main.conf file and restarting the NFS Ganesha service on all protocol nodes.\n",
      "RCA: {\n",
      "  \"RCA\": {\n",
      "    \"incident_id\": \"TS017493063\",\n",
      "    \"date\": \"2024-10-13\",\n",
      "    \"problem\": {\n",
      "      \"description\": \"High failure rate of NVDIMMs in IBM Storage Scale System 5000 with Power9 servers (MTM 5105-22E) running code 6.1.8.3-6.1.9.2.\",\n",
      "      \"symptoms\": \"Customer is concerned about the high failure rate of NVDIMMs.\",\n",
      "      \"detected_by\": \"Customer concern\"\n",
      "    },\n",
      "    \"root_cause\": {\n",
      "      \"description\": \"Not explicitly stated in the provided information, but the customer's concern is related to the high failure rate of NVDIMMs.\",\n",
      "      \"related_services\": [\"IBM Storage Scale System 5000 with Power9 servers (MTM 5105-22E)\"],\n",
      "      \"historical_pattern\": \"Not available in the provided information.\"\n",
      "    },\n",
      "    \"resolution\": {\n",
      "      \"description\": \"Proactive measures and implementation plan to address the high failure rate of NVDIMMs.\",\n",
      "      \"steps_taken\": [\n",
      "        \"Schedule a call to discuss proactive measures and implementation plan.\",\n",
      "        \"Review IBM support article regarding the IBM Storage Scale System 5000 with Power9 servers (MTM 5105-22E) running code 6.1.8.3-6.1.9.2.\"\n",
      "      ],\n",
      "      \"time_to_resolve\": \"Not available in the provided information.\",\n",
      "      \"service_downtime\": \"Not available in the provided information.\",\n",
      "      \"service_restoration\": \"Not available in the provided information.\"\n",
      "    },\n",
      "    \"impact\": {\n",
      "      \"description\": \"Customer concern about the high failure rate of NVDIMMs.\",\n",
      "      \"business_impact\": \"Potential data loss or system instability due to NVDIMM failures.\"\n",
      "    },\n",
      "    \"preventive_actions\": {\n",
      "      \"description\": \"Proactive measures and implementation plan to address the high failure rate of NVDIMMs.\",\n",
      "      \"actions\": [\n",
      "        \"Reboot 'prot3' using the suggested commands.\",\n",
      "        \"Modify the 'MDCACHE' and 'NFS\\_Core\\_Param' blocks in the gpfs.ganesha.main.conf file.\",\n",
      "        \"Restart the NFS Ganesha service on all protocol nodes.\"\n",
      "      ],\n",
      "      \"future_risk\": \"High likelihood of similar issues if the root cause is not addressed.\"\n",
      "    }\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Print Sample RCA\n",
    "i = 0\n",
    "print('Original Summary: ',df['Body'][i])\n",
    "print('RCA:', RCA[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6393e747-e921-4e86-b3d8-db62a00acb1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize vector DB in memory\n",
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.models import VectorParams, Distance\n",
    "\n",
    "client = QdrantClient(\":memory:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d104bfc3-070b-482c-8226-d16f4c4c82b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a collection\n",
    "# Add RCAs to vector database\n",
    "idx = client.add(\n",
    "    collection_name=\"rca_collection\",\n",
    "    documents=RCA\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "42313644-420b-46b7-998f-e7627f1d7d1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score: 0.9092899736736496\n",
      "{\n",
      "  \"RCA\": {\n",
      "    \"incident_id\": \"TS014486964\",\n",
      "    \"date\": \"2024-10-14\",\n",
      "    \"problem\": {\n",
      "      \"description\": \"SSD (2D35-FFF6) detected recoverable error and potential disk drive failure.\",\n",
      "      \"symptoms\": \"Failing function code 722, unknown FRU part number, and disk drive pdisk0/hdisk0 faulty.\",\n",
      "      \"detected_by\": \"Customer service representative (CSR)\"\n",
      "    },\n",
      "    \"root_cause\": {\n",
      "      \"description\": \"Potential disk drive failure in SSD (2D35-FFF6).\",\n",
      "      \"related_services\": [\"rootvg\"],\n",
      "      \"historical_pattern\": \"N/A\"\n",
      "    },\n",
      "    \"resolution\": {\n",
      "      \"description\": \"Replacement of the faulty disk drive.\",\n",
      "      \"steps_taken\": [\n",
      "        \"Analysis of SNAP file to identify the faulty disk drive.\",\n",
      "        \"Removal of the faulty disk from the mirror and volume group prior to replacement.\"\n",
      "      ],\n",
      "      \"time_to_resolve\": \"N/A\",\n",
      "      \"service_downtime\": \"N/A\",\n",
      "      \"service_restoration\": \"N/A\"\n",
      "    },\n",
      "    \"impact\": {\n",
      "      \"description\": \"Potential data loss and system downtime due to disk drive failure.\",\n",
      "      \"business_impact\": \"N/A\"\n",
      "    },\n",
      "    \"preventive_actions\": {\n",
      "      \"description\": \"Regular monitoring of SSD health and timely replacement of failing components.\",\n",
      "      \"actions\": [\n",
      "        \"Implementing proactive monitoring tools for SSD health.\",\n",
      "        \"Establishing a process for timely replacement of failing components.\"\n",
      "      ],\n",
      "      \"future_risk\": \"High likelihood of similar issues if SSD health is not monitored regularly.\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "score: 0.8900585543688107\n",
      "{\n",
      "  \"RCA\": {\n",
      "    \"incident_id\": \"\",\n",
      "    \"date\": \"\",\n",
      "    \"problem\": {\n",
      "      \"description\": \"A physical drive status change was detected in the HP ProLiant DL380 Gen9 server.\",\n",
      "      \"symptoms\": \"Error code 3046 was received, indicating a Smart Array physical drive status change.\",\n",
      "      \"detected_by\": \"IBM Electronic Service Agent (ESA) tool\"\n",
      "    },\n",
      "    \"root_cause\": {\n",
      "      \"description\": \"The root cause is not yet determined as the EED report has not been provided.\",\n",
      "      \"related_services\": [],\n",
      "      \"historical_pattern\": \"\"\n",
      "    },\n",
      "    \"resolution\": {\n",
      "      \"description\": \"The drive will be replaced after receiving the EED report and confirming the replacement with the customer.\",\n",
      "      \"steps_taken\": [\n",
      "        \"Customer was asked to provide the ServiceNow ticket reference and upload the EED report.\"\n",
      "      ],\n",
      "      \"time_to_resolve\": \"\",\n",
      "      \"service_downtime\": \"\",\n",
      "      \"service_restoration\": \"\"\n",
      "    },\n",
      "    \"impact\": {\n",
      "      \"description\": \"The server may require replacement of a physical drive.\",\n",
      "      \"business_impact\": \"Potential downtime and data loss if the issue is not resolved promptly.\"\n",
      "    },\n",
      "    \"preventive_actions\": {\n",
      "      \"description\": \"Regularly monitor the health of the drives and maintain a stock of spare parts.\",\n",
      "      \"actions\": [\n",
      "        \"Implement a process to quickly obtain EED reports when such errors occur.\"\n",
      "      ],\n",
      "      \"future_risk\": \"\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "score: 0.8546524083598923\n",
      "{\n",
      "  \"RCA\": {\n",
      "    \"incident_id\": \"\",\n",
      "    \"date\": \"2024-10-14\",\n",
      "    \"problem\": {\n",
      "      \"description\": \"Failed HPE Smart Storage Battery on HP ProLiant DL380 Gen9 server\",\n",
      "      \"symptoms\": \"Error code 6070, 'cpqHeSysBatteryFailed:The HPE Smart Storage Battery failed.'\"\n",
      "    },\n",
      "    \"root_cause\": {\n",
      "      \"description\": \"Pre-failure state of the smart storage battery\",\n",
      "      \"related_services\": [\"HP ProLiant DL380 Gen9 server\"],\n",
      "      \"historical_pattern\": \"No historical pattern found.\"\n",
      "    },\n",
      "    \"resolution\": {\n",
      "      \"description\": \"Replacement of the failed smart storage battery\",\n",
      "      \"steps_taken\": [\n",
      "        \"Confirmed pre-failure state of the battery.\",\n",
      "        \"Arranged for delivery of replacement battery (part number 878643-001).\",\n",
      "        \"Scheduled engineer to replace the battery.\",\n",
      "        \"Replaced the battery and powered on the server.\",\n",
      "        \"Confirmed that the server was up and running with all green status on the console.\"\n",
      "      ],\n",
      "      \"time_to_resolve\": \"Not specified\",\n",
      "      \"service_downtime\": \"Downtime required\",\n",
      "      \"service_restoration\": \"Server restored after battery replacement.\"\n",
      "    },\n",
      "    \"impact\": {\n",
      "      \"description\": \"Service degradation due to failed battery\",\n",
      "      \"business_impact\": \"No business impact specified.\"\n",
      "    },\n",
      "    \"preventive_actions\": {\n",
      "      \"description\": \"Regularly monitor and maintain the smart storage battery\",\n",
      "      \"actions\": [\n",
      "        \"Schedule regular battery checks and maintenance.\",\n",
      "        \"Keep spare batteries on hand for quick replacements.\"\n",
      "      ],\n",
      "      \"future_risk\": \"High likelihood of similar issues if not properly maintained.\"\n",
      "    }\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Search RCA dataset\n",
    "\n",
    "# query = \"firewall issue\"\n",
    "query = \"a faulty drive was detected\"\n",
    "\n",
    "search_result = client.query(\n",
    "    collection_name=\"rca_collection\",\n",
    "    query_text=query\n",
    ")\n",
    "\n",
    "# Print TOP 3 results\n",
    "for i, r in enumerate(search_result):\n",
    "    print('score:', r.score)\n",
    "    print(r.metadata['document'])\n",
    "    if i+1 >= 3: break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13ab0c73-eda4-44fd-af4f-6c9c0eebadc9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "neuro",
   "language": "python",
   "name": "neuro"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
